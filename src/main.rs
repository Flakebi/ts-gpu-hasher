extern crate grt_amd as geobacter_runtime_amd;

use std::mem::size_of;
use std::ops::*;
use std::time::Instant;

use rand::rngs::SmallRng;
use rand::{Rng, SeedableRng};

use grt_amd::alloc::*;
use grt_amd::module::*;
use grt_amd::signal::*;
use grt_amd::{GeobacterDeps, HsaAmdGpuAccel};
use grt_core::context::Context;

pub type Elem = f32;
const COUNT_MUL: usize = 64;
const COUNT: usize = 1024 * 1024 * COUNT_MUL;
const ITERATIONS: usize = 16;

const WG_SIZE: usize = 256;

fn calc(dest: &mut Elem, value: Elem) {
	let mut i = 0u16;
	while i < ITERATIONS as u16 {
		*dest += 1.0 as Elem;
		*dest *= value;

		i += 1;
	}
}

#[repr(C)] // Ensure we have a universally understood layout
#[derive(GeobacterDeps)]
pub struct Args {
	copy: DeviceSignal,
	tensor: *mut [Elem],
	pub value: Elem,
	queue: DeviceSingleQueue,
	completion: GlobalSignal,
}
impl Completion for Args {
	type CompletionSignal = GlobalSignal;
	fn completion(&self) -> &GlobalSignal { &self.completion }
}
impl Kernel for Args {
	type Grid = Dim1D<Range<u32>>;
	const WORKGROUP: <Self::Grid as GridDims>::Workgroup = Dim1D { x: ..WG_SIZE as _ };
	type Queue = DeviceSingleQueue;

	fn queue(&self) -> &Self::Queue { &self.queue }

	/// This is the kernel that is run on the GPU
	fn kernel(&self, vp: KVectorParams<Self>)
	where Self: Sized {
		if let Some(tensor) = self.tensor_view() {
			let idx = vp.gl_id();
			if let Some(dest) = tensor.get_mut(idx as usize) {
				calc(dest, self.value);
			}
		}
	}
}

impl Args {
	pub fn tensor_view(&self) -> Option<&mut [Elem]> { unsafe { self.tensor.as_mut() } }
}
unsafe impl Send for Args {}
unsafe impl Sync for Args {}

pub fn time<F, R>(what: &str, f: F) -> R
where F: FnOnce() -> R {
	let start = Instant::now();
	let r = f();
	let elapsed = start.elapsed();
	println!("{} took {}ms ({}Î¼s)", what, elapsed.as_millis(), elapsed.as_micros());

	r
}

pub fn main() {
	let ctxt = time("create context", || Context::new().expect("create context"));

	let accels = HsaAmdGpuAccel::all_devices(&ctxt).expect("HsaAmdGpuAccel::all_devices");
	if accels.len() < 1 {
		panic!("no accelerator devices???");
	}

	println!("allocating {} MB of host memory", COUNT * size_of::<Elem>() / 1024 / 1024);

	let lap_alloc = accels.first().unwrap().fine_lap_node_alloc(0);
	let mut original_values: LapVec<Elem> =
		time("alloc original_values", || LapVec::with_capacity_in(COUNT, lap_alloc.clone()));
	original_values.resize_with(COUNT, Default::default);
	let mut values: LapVec<Elem> =
		time("alloc host output slice", || LapVec::with_capacity_in(COUNT, lap_alloc.clone()));
	values.resize_with(COUNT, Default::default);

	let mut rng = SmallRng::from_entropy();

	// run the kernel 20 times for good measure.
	const RUNS: usize = 1;

	for iteration in 0..RUNS {
		println!("Testing iteration {}/{}..", iteration, RUNS);

		for value in original_values.iter_mut() {
			*value = rng.gen();
		}

		for accel in accels.iter() {
			println!("Testing device {}", accel.agent().name().unwrap());

			let mut invoc: FuncModule<Args> = FuncModule::new(&accel);
			invoc.compile_async();
			unsafe {
				invoc.no_acquire_fence();
				invoc.device_release_fence();
			}

			let mut device_values_ptr = time("alloc device slice", || unsafe {
				accel
					.alloc_device_local_slice::<Elem>(COUNT)
					.expect("HsaAmdGpuAccel::alloc_device_local")
			});

			println!(
				"host ptr: 0x{:p}, agent ptr: 0x{:p}",
				original_values.as_ptr(),
				device_values_ptr
			);

			let async_copy_signal = accel
				.new_device_signal(1)
				.expect("HsaAmdGpuAccel::new_device_signal: async_copy_signal");
			let kernel_signal =
				GlobalSignal::new(1).expect("HsaAmdGpuAccel::new_host_signal: kernel_signal");
			let results_signal =
				accel.new_host_signal(1).expect("HsaAmdGpuAccel::new_host_signal: results_signal");

			time("grant gpu access: `original_values` and `values`", || {
				original_values.add_access(&*accel).expect("grant_agents_access");
				values.add_access(&*accel).expect("grant_agents_access");
			});

			unsafe {
				accel
					.unchecked_async_copy_into(
						&original_values,
						&mut device_values_ptr,
						&(),
						&async_copy_signal,
					)
					.expect("HsaAmdGpuAccel::async_copy_into");
			}

			let queue = accel
				.create_single_queue2(None, 0, 0)
				.expect("HsaAmdGpuAccel::create_single_queue");

			let args_pool = time("alloc kernargs pool", || {
				ArgsPool::new::<Args>(&accel, 1).expect("ArgsPool::new")
			});

			const VALUE: Elem = 4.0 as _;
			let args = Args {
				copy: async_copy_signal,
				tensor: device_values_ptr.as_ptr(),
				value: VALUE,
				completion: kernel_signal,
				queue,
			};

			let grid = Dim1D { x: 0u32..COUNT as _ };

			invoc.compile().expect("codegen failed");

			let mut invoc = invoc.into_invoc(&args_pool);

			println!("dispatching...");
			let wait = time("dispatching", || unsafe {
				invoc.unchecked_call_async(&grid, args).expect("Invoc::call_async")
			});

			// specifically wait (without enqueuing another async copy) here
			// so we can time just the dispatch.
			time("dispatch wait", move || {
				wait.wait_for_zero(false).expect("wait for zero failed");
			});

			// now copy the results back to the locked memory:
			unsafe {
				accel
					.unchecked_async_copy_from(
						&device_values_ptr,
						&mut values,
						&(),
						&results_signal,
					)
					.expect("HsaAmdGpuAccel::async_copy_from");
			}
			time("gpu -> cpu async copy", || {
				results_signal.wait_for_zero(true).expect("unexpected signal status");
			});

			let values = nd::aview1(&values);
			let original_values = nd::aview1(&original_values);

			// check results:
			time("checking results", || {
				nd::Zip::from(&values).and(&original_values).par_apply(|&lhs, &rhs| {
					let mut rhs = rhs;
					calc(&mut rhs, VALUE);
					assert_eq!(lhs, rhs);
				});
			});
		}
		println!();
	}
}
